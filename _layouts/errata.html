<!DOCTYPE html>
<html>
  <head>
    <title>{% if page.title %}{{ page.title }} – {% endif %}{{ site.name }} – {{ site.description }}</title>
    {% seo title=false %}
    {% include meta.html %}

    <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="{{ site.name }} - {{ site.description }}" href="{{ site.baseurl }}/feed.xml" />
    <link rel="canonical" href="{{ site.url }}{{ page.url }}" />

    <meta name="theme-color" content="#000000">
    <!-- <link rel="icon" type="image/png" sizes="32x32" href="{{ site.baseurl }}/images/yale.svg"> -->
    <style>
    .collapsible {
      /*background-color: #777;*/
      /*color: white;*/
      /*cursor: pointer;*/
      padding: 18px;
      width: 100%;
      border: none;
      text-align: left;
      outline: none;
      font-size: 17px;
    }

    .active, .collapsible:hover {
      background-color: #ccc;
    }

    .collapsible_content {
      padding: 0 18px;
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.2s ease-out;
      background-color: #f1f1f1;
    }
    </style>


  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <!-- <a href="{{ site.baseurl }}/" class="site-avatar"><img src="{{ site.baseurl }}{{ site.avatar }}" alt="{{ site.title }}" /></a> -->

            <div class="site-info">
              <h1 class="site-name" style="text-align:center">
                <a href="{{ site.baseurl }}/">
                  <b style="text-align:center;">Errata for Algorithms for Convex Optimization</b>
                </a>
              </h1>
              <h3 style="text-align:center;"><a href="{{ site.authorWebsite }}">{{ site.author }}</a></h3>
              <div class="wrapper-main"></div>
            </div>

            <!--<nav>
              <a href="{{ site.baseurl }}/">Home</a>
              <a href="{{ site.baseurl }}/getting-started">Getting Started</a>
              <a href="{{ site.baseurl }}/search">Search</a>
              <a href="{{ site.baseurl }}/about">About</a>
            </nav>-->
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          {{ site.errata }}
         </div>
        <div id="main" role="main" class="container">
        <h2>First print (2021)</h2>
          <!-- <img src="images/polarization.png" height="64" width="64"/> -->

          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 1 - Bridging continuous and discrete optimization </button>
                <div class="collapsible_content">
                  <ol>

                  </ol>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 2 - Preliminaries </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Definition 2.7 (Page 20)<br>
                      <i>&nbsp; Old text:</i> `\sum_{k\geq 3}\nabla^kf(a)[x-a,\ldots,x-a]`<br>
                      <i>New text:</i> `\sum_{k\geq 3}\frac{1}{k!}\nabla^kf(a)[x-a,\ldots,x-a]`<br>
                    </li>

                    <li>
                      Exercise 2.1 (a) (Page 29)<br>
                      <i>&nbsp; Old text:</i> `a_1,\ldots,a_m\in\mathbb{Q}^m`<br>
                      <i>New text:</i> `a_1,\ldots,a_m\in\mathbb{Q}^n`<br>
                    </li>

                    <li>
                      Exercise 2.1 (c) (Page 29)<br>
                      <i>&nbsp; Old text:</i> `A` is a symmetric `n\times n` matrix and `X` runs over symmetric matrices<br>
                      <i>New text:</i> `A` is a real-symmetric `n\times n` matrix and `X` runs over real-symmetric matrices<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 3 - Convexity </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Theorem 3.6 (Page 39)<br>
                      <i>&nbsp; Old text:</i> `f&#58; K\to R`<br>
                      <i>New text:</i> `f&#58; K\to \mathbb{R}`<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 4 - Convex Optimization and Efficiency </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Section 4.1.1 (Page 50)<br>
                      <i>&nbsp; Old text:</i> `b \in \mathbb{R}^n`<br>
                      <i>New text:</i> `b \in \mathbb{R}^m`<br>
                    </li>

                    <li>
                      Exercise 4.9 (c) (Page 66)<br>
                      <i>&nbsp; Old text:</i> vertex of `P`<br>
                      <i>New text:</i> vertex of `K`<br>
                    </li>

                    <li>
                      Exercise 4.11 (c) (Page 67)<br>
                      <i>&nbsp; Old text:</i> polynomial time<br>
                      <i>New text:</i> polynomial time algorithm<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 5 - Duality and Optimality </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Theorem 5.5 (Page 72)<br>
                      <i>&nbsp; Old text:</i> Suppose that the functions `f,f_1,f_2,\ldots,f_m` and ...<br>
                      <i>New text:</i> Suppose that the functions `f,f_1,f_2,\ldots,f_m` are convex and ...<br>
                    </li>

                    <li>
                      Exercise 5.14 (c) (Page 81)<br>
                      <i>&nbsp; Old text:</i> positive orthant<br>
                      <i>New text:</i> non-negative orthant<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 6 - Gradient Descent </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Last equation on Page 92<br>
                      <i>&nbsp; Old text:</i> `R_t \leq `<br>
                      <i>New text:</i> `R_t =`<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 7 - Mirror Descent and  Multiplicative Weights Update </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Page 112<br>
                      <i>&nbsp; Old text:</i> we can ignore terms that depend only on `x`<br>
                      <i>New text:</i> we can ignore terms that depend only on `x^t`<br>
                    </li>

                    <li>
                      Page 112<br>
                      <i>&nbsp; Old text:</i> `f(x^t)` may be more than `f(x^{t+1})`<br>
                      <i>New text:</i> `f(x^{t+1})` may be more than `f(x^{t})`<br>
                    </li>

                    <li>
                      Page 130<br>
                      <i>&nbsp; Old text:</i> `\alpha_e:= \sum_{e\in N(v)} w_v^t`<br>
                      <i>New text:</i> `\alpha_e:= \sum_{v:\ e \in N(v)} w_v^t`<br>
                    </li>

                    <li>
                      Page 131<br>
                      <i>&nbsp; Old text:</i> `\frac{1}{T}\cdot \frac{1}{n}\left(\sum_{e:\ v\in e} x_e^t -1\right)\leq \frac{1}{T}\cdot T\cdot 0 + \delta`<br>
                      <i>New text:</i> `\frac{1}{T}\cdot \sum_{t=0}^{T-1}\frac{1}{n}\left(\sum_{e:\ v\in e} x_e^t -1\right)\leq \frac{1}{T}\cdot T\cdot 0 + \delta`<br>
                    </li>

                    <li>
                      Page 134<br>
                      <i>&nbsp; Old text:</i> `x:= \frac{1}{T}\sum_{t=0}^{T-1}x^i`<br>
                      <i>New text:</i> `x:= \frac{1}{T}\sum_{t=0}^{T-1}x^t`<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 8 - Accelerated Gradient Descent </button>
                <div class="collapsible_content">
                  <ul>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 9 - Newton’s Method </button>
                <div class="collapsible_content">
                  <ul>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 10 - An Interior Point Method for Linear Programming </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Theorem 10.2 (Page 186)<br>
                      <i>&nbsp; Old text:</i> or terminates stating that the polyhedron is infeasible. The algorithm runs in poly`\left(L,\log\frac{1}{\varepsilon}\right)` time.<br>
                      <i>New text:</i> The algorithm runs in poly`\left(L,\log\frac{1}{\varepsilon}\right)` time.<br>
                    </li>

                    <li>
                      Theorem 10.9 (Page 194)<br>
                      <i>&nbsp; Old text:</i> outputs a point `\hat{x}\in P` that satisfies<br>
                      <i>New text:</i> outputs a point `\hat{x}\in`int`(P)` that satisfies<br>
                    </li>

                    <li>
                      Proof of Lemma 10.7 (Page 196)<br>
                      <i>&nbsp; Old text:</i> `n_{\eta'}(x)=H(x)^{-1}\nabla f_{\eta'}(x)`<br>
                      <i>New text:</i> `-n_{\eta'}(x)=H(x)^{-1}\nabla f_{\eta'}(x)`<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 11 - Variants of the Interior Point Method and Self-Concordance </button>
                <div class="collapsible_content">
                  <ul>

                    <li>
                      Equation 11.3 (Page 218)<br>
                      <i>&nbsp; Old text:</i> $$\begin{bmatrix}B & 0\\I & I\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}b\\u\end{bmatrix}$$<br>
                      <i>New text:</i> $$\begin{bmatrix}B & 0\\I & I\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}b\\\rho\end{bmatrix}$$<br>
                    </li>

                    <li>
                      Equation 11.8 (Page 225)<br>
                      <i>&nbsp; Old text:</i> `\tilde{n}_{\eta}(x):=X^2(A^\top (AX^{-2}A^\top)^{-1} AX^2 -I)(\eta c+X^{-1}1)`<br>
                      <i>New text:</i> `\tilde{n}_{\eta}(x):=X^2(A^\top (AX^{2}A^\top)^{-1} AX^2 -I)(\eta c+X^{-1}1)`<br>
                    </li>

                    <li>
                      Lemma 11.10 (Page 228)<br>
                      <i>&nbsp; Old text:</i> `s,t\in G`<br>
                      <i>New text:</i> `s,t\in V`<br>
                    </li>

                    <li>
                      Page 229<br>
                      <i>&nbsp; Old text:</i> `\frac{F}{2}\leq x_{\hat{i}}\leq F=u_{\hat{i}}-F`<br>
                      <i>New text:</i> `\frac{F}{2}\leq x_{\hat{i}}\leq F=\rho_{\hat{i}}-F`<br>
                    </li>

                    <li>
                      Page 229<br>
                      <i>&nbsp; Old text:</i> `\min{\frac{1}{2},\frac{F}{2}} \leq y_i`<br>
                      <i>New text:</i> `\min{\frac{1}{2},F} \leq y_i`<br>
                    </li>

                    <li>
                      Page 231<br>
                      <i>&nbsp; Old text:</i> `||H(x)^{-1}g(x)||_x=g(x)^\top H(x)^{-1} g(x)`<br>
                      <i>New text:</i> `||H(x)^{-1}g(x)||_x^2=g(x)^\top H(x)^{-1} g(x)`<br>
                    </li>

                    <li>
                      Page 231<br>
                      <i>&nbsp; Old text:</i> `||H(x)^{-1}g(x)||_x=1^\top \Pi 1=||\Pi 1||_2^2\leq ||1||_2^2=m`<br>
                      <i>New text:</i> `||H(x)^{-1}g(x)||_x^2=1^\top \Pi 1=||\Pi 1||_2^2\leq ||1||_2^2=m`<br>
                    </li>

                    <li>
                      Exercise 11.10 (Page 244)<br>
                      <i>&nbsp; Old text:</i> self-concordance implies that for all `x\in K`<br>
                      <i>New text:</i> self-concordance implies that for all `x\in`int`(K)`<br>
                    </li>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 12 - Ellipsoid Method for Linear Programming </button>
                <div class="collapsible_content">
                  <ul>

      <li>              
              Section 12.2.2 (Page 257) <br>
              <i>&nbsp; Old text:</i> the method developed we present in  <br>
                      <i>New text:</i>  the method developed in <br>   
                    </li>
                    
                     <li>              
              Section 12.4 (Page 262) <br>
              <i>&nbsp; Old text:</i> The proof of Lemma 12.12 is has two parts   <br>
                      <i>New text:</i> The proof of Lemma 12.12 has two parts  <br>   
                    </li>
                    
                     <li>              
             Algorithm 9 (Page 253) <br>
              <i>&nbsp; Old text:</i> A number `u` such that `u\leq y^\star+\varepsilon` <br>
                      <i>New text:</i> A number `u` such that `y^\star \leq u\leq y^\star+\varepsilon` <br>   
                    </li>
                    
                     <li>              
              Proof of Theorem 12.7 (Page 256) <br>
              Replace `n` by `m`  <br>   
                    </li>
                    
                    
                     <li>              
              Exercise 12.3 (Page 274) <br>
              Replace `n` by `m`  <br> 
                    </li>
                    
                    
        <li>              
              Section 12.2.2 (Page 257) <br>
              <i>&nbsp; Old text:</i>  <br>
                      <i>New text:</i>  <br>   
                    </li>


                  </ul>
                </div>

              </li>
          	</ul>
          </div>


          <div style='width:100%; height:auto;'>
          	<ul>
          		<li> <button class="collapsible"> Chapter 13 - Ellipsoid Method for Convex Optimization </button>
                <div class="collapsible_content">
                  <ul>

                  </ul>
                </div>

              </li>
          	</ul>
          </div>

        </div>
      </div>




      <!-- <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <h2> Copyright</h2>
          {{ site.copyright }}
          <br>
        </div>
      </div> -->




       <!-- <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <h2> Other books by the author</h2>
         <ul>
          		<li> <a href=http://cs.yale.edu/homes/vishnoi/Lxb-Web.pdf>Lx=b</a>

              </li> <br>
           <li> <a href=http://www.cs.yale.edu/homes/vishnoi/Publications_files/approx-survey.pdf>Faster algorithms via approximation theory</a>

              </li>
          	</ul>
         </div>
      </div> -->
      <div class="wrapper-main"></div>
      <div class="wrapper-main"></div>
      <!-- <div class="wrapper-main">
        <div id="main" role="main" class="container">
            <h2> About the author</h2>
            {{ site.bio }}
           <div style="margin-bottom:3cm;">
           </div>
        </div>
      </div> -->


      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            {% include svg-icons.html %}
          </footer>
        </div>
      </div>
    </div>

  </body>

   <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight){
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight + "px";
        }
      });
    }


    </script>

</html>
<script>
MathJax = {
  loader: {load: ['input/asciimath', 'output/chtml']}
}
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
