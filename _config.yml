#
# This file contains configuration flags to customize your site
#

# Name of your site (displayed in the header)
name: Algorithms for Convex Optimization

# Short bio or description (displayed in the header)
description: Convex optimization studies the problem of minimizing a convex function over a convex set. Convexity, along with its numerous implications, has been used to come up with efficient algorithms for many classes of convex programs. Consequently, convex optimization has broadly impacted several disciplines of science and engineering.<br> <br> In the last few years, algorithms for convex optimization have revolutionized algorithm design, both for discrete and continuous optimization problems. The fastest known algorithms for problems such as maximum flow in graphs, maximum matching in bipartite graphs, and submodular function minimization, involve an essential and nontrivial use of algorithms for convex optimization such as gradient descent, mirror descent, interior point methods, and cutting plane methods. Surprisingly, algorithms for convex optimization have also been used to design counting problems over discrete objects such as matroids. Simultaneously, algorithms for convex optimization have become central to many modern machine learning applications. The demand for algorithms for convex optimization, driven by larger and increasingly complex input instances, has also significantly pushed the state of the art of convex optimization itself.  <br> <br> The goal of this book is to enable a reader to gain an in depth understanding of algorithms for convex optimization. The emphasis is to derive key algorithms for convex optimization from first principles and to establish precise running time bounds in terms of the input length. Given the broad applicability of these methods, it is not possible for a single book to show the applications of these methods to all of them. This book shows applications to fast algorithms for various discrete optimization and counting problems. The applications selected in this book serve the purpose of illustrating a rather surprising bridge between continuous and discrete optimization.<br><br> The intended audience includes advanced undergraduate students, graduate students, and researchers from theoretical computer science, discrete optimization, and machine learning.

# Short author bio
bio: Nisheeth Vishnoi is a A. Bartlett Giamatti Professor of Computer Science and a co-founder of the <a href=https://computationsociety.yale.edu/>Computation and Society Initiative</a> at Yale University.  His research focuses on foundational problems in computer science, machine learning, and optimization.  He is also broadly interested in understanding and addressing some of the key questions that arise in nature and society from a computational viewpoint. Here, his current focus is on natural algorithms, the emergence of intelligence, and fairness in algorithmic decision making.<br><br> He is the recipient of the Best Paper Award at IEEE FOCS in 2005, the IBM Research Pat Goldberg Memorial Award in 2006, the Indian National Science Academy Young Scientist Award in 2011, the IIT Bombay Young Alumni Achievers Award in 2016,  and the Best Technical Paper award at ACM FAT* in 2019. He is a Fellow of the ACM.<br><br> He holds a B. Tech., Computer Science and Engineering, Indian Institute of Technology Bombay, 1995-1999, Ph. D., Algorithms, Combinatorics and Optimization, Georgia Institute of Technology, 1999-2004.


# URL of your avatar or profile pic (you could use your GitHub profile pic)
avatar: images/blank.png

pdf: ACO-v1.pdf


author: Nisheeth K. Vishnoi
authorWebsite: https://www.cs.yale.edu/homes/vishnoi/Home.html

chapters:
    - number: 1
      title: Bridging continuous and discrete optimization
      description: We present the interplay between continuous and discrete optimization. The motivating example is that of the maximum flow problem. We also trace the history of linear programming - from the ellipsoid method to modern interior point methods. We end with some of the recent successes of the ellipsoid method for general convex programming problems such as the maximum entropy problem.

    - number: 2
      title: Preliminaries
      description: We review the mathematical preliminaries required for this book. These include some standard notion and facts from multivariate calculus, linear algebra, geometry, topology, dynamical systems, and graph theory.

    - number: 3
      title: Convexity
      description: 'We introduce convex sets, notions of convexity, and show the power that comes along with convexity: convex sets have separating hyperplanes, subgradients exist, and locally optimal solutions of convex functions are globally optimal.'

    - number: 4
      title: Convex Optimization and Efficiency
      description: We present the notion of convex optimization and discuss formally what it means to solve a convex program efficiently as a function of the representation length of the input and the desired accuracy.

    - number: 5
      title: Duality and Optimality
      description: We introduce the notion of Lagrangian duality and show that under a mild condition, called Slater’s condition, strong Lagrangian duality holds. Subsequently, we introduce the Legendre-Fenchel dual that often arises in Lagrangian duality and optimization methods. Finally, we present Kahn-Karush-Tucker (KKT) optimality conditions and their relation to strong duality.


    - number: 6
      title: Gradient Descent
      description: 'We start by presenting the gradient descent method and show how it can be viewed as a steepest descent. Subsequently, we prove a convergence time bound on the gradient descent method when the gradient of the function is Lipschitz continuous. Finally, we use the gradient descent method to come up with a fast algorithm for a discrete optimization problem: computing maximum flow in an undirected graph.'

    - number: 7
      title: Mirror Descent and  Multiplicative Weights Update
      description: We derive our second algorithm for convex optimization – called the mirror descent method – via a regularization viewpoint. First, the mirror descent algorithm is developed for optimizing convex functions over the probability simplex. Subsequently, we show how to generalize it and, importantly, derive the multiplicative weights update (MWU) method from it. This latter algorithm is then used to develop a fast approximate algorithm to solve the bipartite matching problem on graphs.


    - number: 8
      title: Accelerated Gradient Descent
      description: We present Nesterov’s accelerated gradient descent algorithm. This algorithm can be viewed as a hybrid of the previously introduced gradient descent and mirror descent methods. We also present an application of the accelerated gradient method to solving a linear system of equations.


    - number: 9
      title: Newton’s Method
      description: We begin our journey towards designing algorithms for convex optimization whose number of iterations scale polylogarithmically with the error. As a first step, we derive and analyze the classic Newton’s method, which is an example of a second-order method. We argue that Newton’s method can be seen as steepest descent on a Riemannian manifold, which then motivates an affinely invariant analysis of its convergence.

    - number: 10
      title: An Interior Point Method for Linear Programming
      description: We build upon Newton’s method and its convergence to derive a polynomial time algorithm for linear programming. Key to the this algorithm is a reduction from constrained to unconstrained optimization using the notion of a barrier function and the corresponding central path.

    - number: 11
      title: Variants of the Interior Point Method and Self-Concordance
      description: We present various generalizations and extensions of the path following IPM for the case of linear programming. As an application, we derive a fast algorithm for the s-t-minimum cost flow problem. Subsequently, we introduce the notion of self-concordance and give an overview of barrier functions for polytopes and more general convex sets.

    - number: 12
      title: Ellipsoid Method for Linear Programming
      description: We introduce a class of cutting plane methods for convex optimization and present an analysis of a special case, namely, the ellipsoid method. We then show how to use this ellipsoid method to solve linear programs over 0-1-polytopes when we only have access to a separation oracle for the polytope.

    - number: 13
      title: Ellipsoid Method for Convex Optimization
      description: We show how to adapt the ellipsoid method to solve general convex programs. As applications, we present a polynomial time algorithm for submodular function minimization and a polynomial time algorithm to compute maximum entropy distributions over combinatorial polytopes.



copyright: This material will be published by Cambridge University Press as Algorithms for Convex Optimization by Nisheeth K. Vishnoi. This pre-publication version is free to view and download for personal use only. Not for re-distribution, re-sale or use in derivative works. &#169; Nisheeth K. Vishnoi 2020.

errata: Feedback, corrections, and comments are welcome and should be emailed to the author.

errataPage: errata/index.html

#
# Flags below are optional
#

# Includes an icon in the footer for each username you enter
footer-links:
  dribbble:
  email:
  facebook:
  flickr:
  github:
  instagram:
  linkedin:
  pinterest:
  rss:
  twitter:
  stackoverflow:
  youtube: # channel/<your_long_string> or user/<user-name>
  googleplus: # anything in your profile username that comes after plus.google.com/
  playconsole:


# Enter your Disqus shortname (not your username) to enable commenting on posts
# You can find your shortname on the Settings page of your Disqus account
disqus:

# Enter your Google Analytics web tracking code (e.g. UA-2110908-2) to activate tracking
# google_analytics: UA-43339302-11

# Your website URL (e.g. http://amitmerchant1990.github.io or http://www.amitmerchant.com)
# Used for Sitemap.xml and your RSS feed
#url: http://www.amitmerchant.com/reverie
#enforce_ssl: https://www.amitmerchant.com/reverie

# If you're hosting your site at a Project repository on GitHub pages
# (http://yourusername.github.io/repository-name)
# and NOT your User repository (http://yourusername.github.io)
# then add in the baseurl here, like this: "/repository-name"
#baseurl: ""

#
# !! You don't need to change any of the configuration flags below !!
#

permalink: /:title/

# The release of Jekyll Now that you're using
version: v1.2.0

# Jekyll 3 now only supports Kramdown for Markdown
kramdown:
  # Use GitHub flavored markdown, including triple backtick fenced code blocks
  input: GFM
  # Jekyll 3 and GitHub Pages now only support rouge for syntax highlighting
  syntax_highlighter: rouge
  syntax_highlighter_opts:
    # Use existing pygments syntax highlighting css
    css_class: 'highlight'

# Set the Sass partials directory, as we're using @imports
sass:
  style: :expanded # You might prefer to minify using :compressed

# Use the following plug-ins
plugins:
  - jekyll-sitemap # Create a sitemap using the official Jekyll sitemap gem
  - jekyll-feed # Create an Atom feed using the official Jekyll feed gem
  - jekyll-seo-tag
  - jekyll-paginate

include: ['_pages']

paginate: 6
paginate_path: /page:num/

# Exclude these files from your production _site
exclude:
  - Gemfile
  - Gemfile.lock
  - LICENSE
  - README.md
  - CNAME

